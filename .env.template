# Copy this file to `.env` and fill in your actual values.

# LlumDocs loads everything via `llumdocs.llm` and FastAPI / Gradio settings.

# ============================================================================
# 1. OpenAI / LiteLLM configuration
# ============================================================================

# API key for OpenAI-compatible providers (LiteLLM will route calls).
# If unset, only Ollama models will be used (when available).
OPENAI_API_KEY=your-openai-api-key-here

# Preferred text model id when no explicit `model_hint` is provided.
# Examples: gpt-4o-mini, gpt-4o, gpt-3.5-turbo, ollama/llama3.1:8b
LLUMDOCS_DEFAULT_MODEL=gpt-4o-mini

# Preferred vision model id for image description tasks.
# Examples: o4-mini, gpt-4.1-mini, ollama/qwen3-vl:8b
LLUMDOCS_DEFAULT_VISION_MODEL=o4-mini

# Timeout for text LLM calls in seconds (default: 30.0).
# Increase this if you experience timeout errors with slower models.
LLUMDOCS_LLM_TIMEOUT_SECONDS=30.0

# Timeout for vision/image model calls in seconds (default: 60.0).
# Vision models are typically slower, especially on CPU or limited GPU memory.
# Falls back to LLUMDOCS_LLM_TIMEOUT_SECONDS if set, otherwise defaults to 60.0.
# For Docker with GPU, 60-90 seconds is usually sufficient.
# For CPU-only or limited GPU, you may need 120+ seconds.
LLUMDOCS_VISION_TIMEOUT_SECONDS=120.0

# ============================================================================
# 2. Ollama configuration (optional)
# ============================================================================

# Base URL for a local Ollama server.
# In docker-compose we typically point this to http://ollama:11434
# default (bare metal): http://localhost:11434
# OLLAMA_API_BASE=http://localhost:11434

# If set, applies Ollama's server-side default; our client requests still force keep_alive=0.
# Examples: "0" (unload immediately), "300s" (5 minutes), "-1" (forever)
# Default: unset (client requests explicitly set keep_alive=0)
OLLAMA_KEEP_ALIVE=0

# Set to "1" to completely disable using Ollama models, even if reachable.
LLUMDOCS_DISABLE_OLLAMA=0

# ============================================================================
# 3. FastAPI API configuration
# ============================================================================

# Comma-separated list of allowed origins for CORS.
# Examples:
#   http://localhost:3000
#   http://localhost:3000,http://127.0.0.1:3000
# For local Gradio UI, this is enough:
LLUMDOCS_CORS_ORIGINS=http://localhost:7860

# Host / port for the FastAPI server (used by uvicorn in Docker).
LLUMDOCS_HOST=0.0.0.0
LLUMDOCS_PORT=8000

# Enable auto-reload in development (DO NOT use in production).
# Values: true/false/1/0/yes/no
LLUMDOCS_RELOAD=false

# ============================================================================
# 4. Gradio UI configuration (optional)
# ============================================================================

# Host / port where the Gradio UI will bind.
LLUMDOCS_UI_HOST=0.0.0.0
LLUMDOCS_UI_PORT=7860

# Whether to use Gradio "share" URLs (useful for demos, not prod).
LLUMDOCS_UI_SHARE=false

# ============================================================================
# 5. Email Intelligence (optional, HuggingFace models)
# ============================================================================

# These models are loaded lazily by llumdocs.services.email_intelligence_service.

# Enable/disable email intelligence feature (default: 1, enabled).
# Set to "0" to disable email intelligence if you do not need it.

LLUMDOCS_ENABLE_EMAIL_INTELLIGENCE=1

# Model configuration (defaults match the INSTALL.md docs):

# Override default zero-shot routing model for email intelligence.
# Default: MoritzLaurer/bge-m3-zeroshot-v2.0
LLUMDOCS_EMAIL_ZEROSHOT_MODEL=MoritzLaurer/bge-m3-zeroshot-v2.0

# Override default phishing detection model for email intelligence.
# Default: cybersectony/phishing-email-detection-distilbert_v2.1
LLUMDOCS_EMAIL_PHISHING_MODEL=cybersectony/phishing-email-detection-distilbert_v2.1

# Override default sentiment analysis model for email intelligence.
# Default: cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual
LLUMDOCS_EMAIL_SENTIMENT_MODEL=cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual

# Maximum number of tokens sent to Hugging Face email models (default 512).
# Increase if you switch to models that support longer contexts.
LLUMDOCS_EMAIL_MAX_TOKENS=512

# Hugging Face cache directory (used for all HF models).
# In Docker we point this to a volume, e.g. /models/hf
# If unset, HF_HOME / ~/.cache/huggingface will be used.
HF_HOME=/models/hf

# ============================================================================
# 6. Test suite helpers (integration tests)
# ============================================================================

# Comma-separated list of LiteLLM ids used in live text tests.
# Examples: gpt-4o-mini,ollama/llama3.1:8b
# If left empty, integration tests are automatically skipped.
LLUMDOCS_LIVE_TEST_MODELS=gpt-4o-mini,ollama/llama3.1:8b

# Comma-separated list of LiteLLM ids used in live vision tests.
# Examples: o4-mini,ollama/qwen3-vl:8b
# If left empty, integration tests are automatically skipped.
LLUMDOCS_LIVE_TEST_VISION_MODELS=o4-mini,ollama/qwen3-vl:8b
