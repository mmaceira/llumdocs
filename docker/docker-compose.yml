version: "3.9"

services:
  app:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: llumdocs
    env_file:
      - ../.env
    environment:
      PYTHONUNBUFFERED: "1"
      SERVER_NAME: 0.0.0.0
      SERVER_PORT: "7860"
      SHARE: "false"
      # UI -> API (same container)
      LLUMDOCS_API_URL: http://localhost:8000
      # App (API/services) -> Ollama over Docker DNS
      OLLAMA_API_BASE: http://ollama:11434
      HF_HOME: /models/hf
      GRADIO_ALLOW_REMOTE_CONNECTIONS: "true"
      GRADIO_SERVER_NAME: 0.0.0.0
      GRADIO_SKIP_PRELAUNCH_CHECK: "true"
      LLUMDOCS_GRADIO_SHOW_API: "false"
    command: ["/bin/sh", "-lc", "wait-for-ollama && uv run honcho -f docker/Procfile start"]
    ports:
      - "7860:7860"
      - "8000:8000"
    depends_on:
      - ollama
    restart: unless-stopped

  ollama:
    image: ollama/ollama:0.13.0
    environment:
      # canonical form (no http:// prefix)
      OLLAMA_HOST: 0.0.0.0:11434
      # Tell Ollama about available memory (container limit minus overhead)
      OLLAMA_MAX_LOADED_MODELS: 1
      OLLAMA_NUM_PARALLEL: 1
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped
    mem_limit: 8g
    mem_reservation: 6g

volumes:
  ollama_models:
