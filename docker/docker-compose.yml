version: "3.9"

services:
  # ==============================================================
  # FastAPI API (CPU-only base image)
  # ==============================================================
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: api
    env_file:
      - ../.env
    environment:
      # Use Docker Ollama service (internal network)
      # To use host Ollama instead, set OLLAMA_API_BASE=http://host.docker.internal:11434 in .env
      OLLAMA_API_BASE: ${OLLAMA_API_BASE:-http://ollama:11434}
      HF_HOME: /models/hf
    volumes:
      # Cache for HuggingFace models (CPU-mode)
      - hf_cache:/models/hf
    ports:
      - "8000:8000"
    # Ollama dependency is optional - can use host Ollama instead
    # depends_on:
    #   - ollama
    profiles:
      # Start this API in CPU, GPU and UI profiles
      - cpu
      - gpu
      - ui

  # ==============================================================
  # FastAPI API with bundled HuggingFace models
  # (alternative to api, not meant to run at the same time)
  # ==============================================================
  api-hf:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: api_hf
    env_file:
      - ../.env
    environment:
      OLLAMA_API_BASE: http://ollama:11434
      HF_HOME: /models/hf
    volumes:
      # Optional: still mount to persist across container rebuilds
      - hf_cache:/models/hf
    ports:
      # Use the same external port, but with a separate profile so it doesn't
      # conflict with `api`. Only one of api / api-hf should be running.
      - "8000:8000"
    depends_on:
      - ollama
    profiles:
      - hf-bundled

  # ==============================================================
  # Gradio UI
  # ==============================================================
  ui:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: ui
    env_file:
      - ../.env
    environment:
      # Use Docker Ollama service (internal network)
      OLLAMA_API_BASE: ${OLLAMA_API_BASE:-http://ollama:11434}
    ports:
      - "7860:7860"
    depends_on:
      api:
        condition: service_started
      ollama:
        condition: service_healthy
    profiles:
      - ui

  # ==============================================================
  # Ollama (local LLM server for text + vision models)
  # ==============================================================
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    volumes:
      - ollama_models:/root/.ollama
      # Mount entrypoint script to auto-pull models
      - ./entrypoint-ollama.sh:/entrypoint-ollama.sh:ro
    ports:
      # Use different port to avoid conflict with host Ollama
      # If host Ollama is not running, you can change this back to 11434:11434
      - "11435:11434"
    environment:
      # Keep models warmed longer; tweak as needed
      OLLAMA_KEEP_ALIVE: 24h
    entrypoint: ["/bin/bash", "/entrypoint-ollama.sh"]
    healthcheck:
      # Verify Ollama is responding (entrypoint script ensures models are downloaded)
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 600s  # Give up to 10 minutes for models to download on first start
    profiles:
      # Use Ollama in all relevant profiles
      - cpu
      - gpu
      - hf-bundled
    # GPU support is disabled by default in the main compose file.
    # For GPU support, either:
    # 1. Install NVIDIA Container Toolkit and use docker-compose.gpu.yml
    # 2. Or manually add: runtime: nvidia (after installing NVIDIA Container Toolkit)

volumes:
  ollama_models:
  hf_cache:
